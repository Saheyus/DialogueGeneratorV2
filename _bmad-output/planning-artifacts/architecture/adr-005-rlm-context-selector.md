# ADR-005: RLM Context Selector (Autonomous Context Selection)

**Date:** 2026-01-17  
**Status:** âœ… Proposed  
**Deciders:** Architecture Team  
**Tags:** #context-selection #llm #rlm #gdd #optimization

---

## Context

SÃ©lection manuelle de contexte GDD est **cognitivement coÃ»teuse et error-prone** :

- ScÃ¨ne "minimale" (2 personnages + 1 lieu) fait dÃ©jÃ  **15-20k tokens** en mode full
- Utilisateur doit dÃ©cider manuellement quelles fiches inclure et en quel mode (full/excerpt)
- Risque d'oublier Ã©lÃ©ments pertinents (liens cosmologiques, factions, objets rituels)
- GranularitÃ© trop grossiÃ¨re : fiche "full" = 6-8k tokens, mÃªme si seule une section est pertinente

**ProblÃ¨me fondamental :** Avec des contextes de 20k+ tokens, mÃªme avec fenÃªtres 128k, les effets de dÃ©gradation OOLONG apparaissent (attention diluÃ©e, dÃ©pendances longues brouillÃ©es, rappel prÃ©cis dÃ©gradÃ©). Le vrai problÃ¨me n'est pas "comment choisir quelles fiches charger" mais **"comment raisonner sur un univers dont la scÃ¨ne active pÃ¨se dÃ©jÃ  20k tokens"**.

**RÃ©fÃ©rence acadÃ©mique :** Recursive Language Models (RLM) - arXiv:2512.24601 - Paradigme oÃ¹ le LLM ne "voit" jamais tout le contexte, mais navigue, lit par tranches, rÃ©sume, rÃ©-interroge, vÃ©rifie localement, agrÃ¨ge rÃ©cursivement.

---

## Decision

ImplÃ©menter une **couche optionnelle (on/off) de LLM "sÃ©lecteur autonome de contexte"** inspirÃ©e du paradigme **Recursive Language Models (RLM)** :

- Le systÃ¨me devient l'agent de sÃ©lection (exploration programmatique + dÃ©ductions)
- L'utilisateur devient superviseur (valide/ajuste, avec mode override)
- RÃ©duction contextuelle intelligente : 20k+ tokens â†’ 12-15k tokens sans perte de pertinence

---

## Technical Design

### Phase 1. Context Selection (RLM Agent)

**Service Backend:**
```python
# services/rlm_context_selector.py
class RLMContextSelector:
    async def select_context(
        self,
        user_instructions: str,  # Instructions de ScÃ¨ne
        hints: Optional[Dict[str, List[str]]] = None,  # Optionnel : verrouiller Ã©lÃ©ments
        hints_mode: Optional[Dict[str, str]] = None,  # {"character_A": "full"}
        exclude: Optional[List[str]] = None,  # IDs Ã  exclure
        expansion_radius: int = 1,  # 0=aucune, 1=graphe direct, 2=indirect
        max_tokens_target: int = 15000,  # Budget global
        seed: Optional[int] = None,  # ReproductibilitÃ©
    ) -> ContextSelectionResult:
        # 1. Parse user_instructions pour extraire entitÃ©s explicites
        # 2. Exploration outillÃ©e (search_bm25, get_related, get_snippet, etc.)
        # 3. DÃ©ductions (liens cosmologiques, factions, objets rituels, etc.)
        # 4. DÃ©cision full/excerpt + section_filters pour chaque fiche
        # 5. Budget check (si dÃ©passement, passer plus en excerpt ou exclure)
        # 6. Retourner selected_elements + justifications + trace
```

**Outils GDD (exposÃ©s au LLM via function calling via `GDDToolsProvider`):**
```python
# Outils de navigation JSON
- get_node(id) -> json
- get_fields(id, fields[]) -> json
- list_ids(type=None, where_field_exists=None, limit=...)
- schema_overview() -> stats + exemples

# Outils de recherche
- search_bm25(query, top_k=20, filter_type=None) -> [{id, score, snippet}]
- search_regex(pattern, field=None, top_k=20) -> matches
- search_by_key_value(key, value, exact=True)

# Outils d'extraction contrÃ´lÃ©e
- get_snippet(id, field, max_chars=2000, around=None)
- get_related(id, relation_keys=[...], depth=1)
- get_relation_chunks(source_id, target_id, relation_field="Relations") -> Dict[str, Any]  # <-- NOUVEAU: Chunks ciblÃ©s

# Outils d'agrÃ©gation
- count(filter...)
- group_by(field, filter...)
- build_table(ids, columns) -> rows
- diff(id_a, id_b, fields)
```

**Limites RLM Agent:**
```python
MAX_TOOL_CALLS = 50  # Limite absolue appels outils
MAX_EXPLORATION_TOKENS = 100000  # Budget max exploration (modÃ¨le mini, coÃ»t faible)
```

**Output Phase 1:**
```python
{
  "selected_elements": {
    "characters": {
      "UresaÃ¯r": {
        "mode": "full",
        "section_filters": {
          "include": ["Psychologie", "Arc.Actuel", "Relations.Akthar"],
          "exclude": ["RÃ´le cosmologique complet", "Histoire complÃ¨te"],
          "reason": "Focus sur dynamique relationnelle et Ã©tat Ã©motionnel"
        },
        "field_filters": {  # <-- NOUVEAU: GranularitÃ© chunks ciblÃ©s (Phase 2)
          "Relations": {
            "mode": "intersection",
            "related_elements": ["Akthar"],
            "reason": "Extrait uniquement relations communes entre UresaÃ¯r et Akthar"
          }
        },
        "justification": {
          "reason": "hint_explicit",
          "proof": None
        }
      },
      "Akthar": {
        "mode": "full",
        "section_filters": {
          "include": ["Psychologie", "Relations.UresaÃ¯r", "Croyances"],
          "exclude": ["RÃ´le cosmologique complet"]
        },
        "justification": {
          "reason": "hint_explicit",
          "proof": None
        }
      }
    },
    "locations": {
      "Nef Centrale": {
        "mode": "full",
        "section_filters": {...},
        "justification": {
          "reason": "mentioned_explicitly",
          "proof": "ScÃ¨ne se dÃ©roule dans la Nef Centrale"
        }
      },
      "LÃ©viathan PÃ©trifiÃ©": {
        "mode": "excerpt",
        "justification": {
          "reason": "deduction_context_cosmologique",
          "proof": "LÃ©viathan mentionnÃ© comme cadre cosmologique dans UresaÃ¯r.sections.RÃ´le",
          "search_trace": ["get_related('UresaÃ¯r')", "search_by_key_value('type', 'lieu_cosmologique')"]
        }
      }
    }
  },
  "trace": {
    "tools_called": ["search_bm25", "get_related", "get_snippet", ...],
    "decisions": [...],
    "total_tokens_estimated": 12000  # OptimisÃ© vs 20k+ en manuel
  }
}
```

### Phase 2. Context Build (inchangÃ© mais enrichi)

**Integration avec ContextFieldManager:**
```python
# services/context_field_manager.py
def filter_fields_by_section_filters(
    self,
    element_type: str,
    fields_to_include: List[str],
    section_filters: Optional[Dict[str, Any]] = None  # <-- ENRICHI: include/exclude + field_filters
) -> List[str]:
    """
    section_filters peut contenir:
    {
      "include": ["Relations.Akthar"],      # Sous-sections Ã  inclure
      "exclude": ["RÃ´le cosmologique"],     # Sous-sections Ã  exclure
      "field_filters": {                     # NOUVEAU (Phase 2): Chunks ciblÃ©s
        "Relations": {
          "mode": "intersection",            # Relations communes
          "related_elements": ["Akthar"]
        }
      }
    }
    """
    # 1. Appliquer include/exclude (niveau sous-section) âœ…
    # 2. Appliquer field_filters (niveau chunk) si prÃ©sent ðŸ†•
    # 3. Extraire chunks ciblÃ©s via get_relation_chunks() si field_filters ðŸ†•
    # Combine rÃ¨gles statiques (context_config.json) + rÃ¨gles dynamiques (section_filters)
    # Sans bypasser le DSL de champs existant
```

**Backend API:**
```python
# api/routers/context.py
@router.post("/select-context", response_model=SelectContextResponse)
async def select_context_auto(
    request_data: SelectContextRequest,
    rlm_selector: Annotated[RLMContextSelector, Depends(get_rlm_context_selector)],
) -> SelectContextResponse:
    # Phase 1 : RLM sÃ©lection automatique
    selection_result = await rlm_selector.select_context(
        user_instructions=request_data.user_instructions,
        hints=request_data.hints,
        ...
    )
    # Phase 2 : build_context_json (inchangÃ©)
    structured_context = context_builder.build_context_json(
        selected_elements=selection_result.selected_elements,
        scene_instruction=request_data.user_instructions,
        ...
    )
    return SelectContextResponse(
        selected_elements=selection_result.selected_elements,
        context=structured_context,
        trace=selection_result.trace
    )
```

**Frontend UI:**
- Toggle "Auto Selection" (on/off) dans panneau contexte
- Affichage "Contexte auto-sÃ©lectionnÃ©" avec justifications cliquables
- Mode "Override" : utilisateur peut forcer/ajouter des Ã©lÃ©ments mÃªme en auto
- Mode "Lock" : utilisateur peut verrouiller certains Ã©lÃ©ments (toujours inclus)

---

## Constraints

- **DOIT** Ãªtre optionnel (on/off), avec fallback vers sÃ©lection manuelle
- **DOIT** rester compatible avec `ContextFieldManager`, `ContextTruncator`, `ContextSerializer`
- **NE DOIT PAS** bypasser `build_context_json()` (Option A, pas Option B)
- **DOIT** produire `selected_elements` avec `section_filters` enrichis
- **DOIT** inclure `justification` et `trace` pour traÃ§abilitÃ©
- **DOIT** respecter hints explicites (toujours inclus, mode full par dÃ©faut)
- **DOIT** Ãªtre reproductible (seed optionnel) ou au minimum traÃ§able
- **DOIT** gÃ©rer fallback gracieux (si RLM Ã©choue, retourner hints uniquement, pas d'erreur)

---

## Rationale

### RÃ©duction friction
- Plus besoin de sÃ©lection manuelle laborieuse (10+ clics â†’ 1 clic "Auto")
- AmÃ©lioration recall : RLM trouve Ã©lÃ©ments pertinents que l'utilisateur aurait oubliÃ©s

### GranularitÃ© adaptative
- **Phase 1 (MVP)** : SÃ©lection fine de sous-sections (ex: UresaÃ¯r 6k â†’ 2-3k tokens) sans perte pertinence
- **Phase 2 (AmÃ©lioration)** : Chunks ciblÃ©s dans sous-sections (ex: relations communes entre 2 personnages, 200 tokens vs 2000 tokens pour "Relations" complet)
- RÃ©duction contextuelle intelligente : 20k+ â†’ 12-15k tokens (Phase 1) â†’ 6-10k tokens (Phase 2 avec chunks)

### Paradigme RLM
- Navigation programmatique du GDD, lecture rÃ©cursive, mÃ©moire de travail compacte, agrÃ©gation progressive
- RÃ©sout problÃ¨me fondamental : contexte massif â†’ environnement informationnel navigable

### Compatible existant
- S'intÃ¨gre proprement avec `ContextBuilder` sans casser invariants
- Phase 2 utilise toujours `build_context_json()` (pas de bypass)

---

## Risks

### Non-dÃ©terminisme
- Agent peut choisir trajectoire diffÃ©rente (mitigation : seed + cache + traÃ§abilitÃ©)

### SÃ©lection inattendue
- RLM peut inclure Ã©lÃ©ments non souhaitÃ©s (mitigation : override + lock + exclusions)

### CoÃ»t LLM
- Exploration outillÃ©e = plusieurs appels LLM (mitigation : budget sÃ©parÃ© 100k tokens + cache + modÃ¨le GPT-5-mini pour sÃ©lection)

### Latence
- SÃ©lection automatique ajoute dÃ©lai avant gÃ©nÃ©ration (mitigation : cache + streaming progress)

### Tests
- Agent loop difficile Ã  tester sans fixtures synthÃ©tiques (mitigation : tests avec mini-GDD + mocks LLM)

---

## Tests Required

### Unit
- `RLMContextSelector.select_context()` avec mocks LLM
- `ContextFieldManager.filter_fields_by_section_filters()` combine rÃ¨gles

### Integration
- `/api/v1/context/select-context` avec vrai LLM (tests coÃ»teux, limiter)
- Fallback gracieux si RLM Ã©choue

### E2E
- Workflow complet auto-selection â†’ build_context â†’ gÃ©nÃ©ration

---

## Acceptance Criteria

- [ ] Toggle "Auto Selection" dans UI contexte
- [ ] RLM produit `selected_elements` avec `section_filters`
- [ ] Phase 2 `build_context_json()` utilise `section_filters` correctement
- [ ] RÃ©duction tokens : 20k+ â†’ 12-15k sans perte pertinence
- [ ] Justifications affichÃ©es (utilisateur peut comprendre pourquoi Ã©lÃ©ment inclus)
- [ ] Mode override fonctionne (ajout/force Ã©lÃ©ments mÃªme en auto)
- [ ] Fallback gracieux si RLM Ã©choue (pas d'erreur, retourne hints uniquement)
- [ ] TraÃ§abilitÃ© complÃ¨te (trace contient trajectoire agent)

---

## Open Questions (RÃ©solues)

1. **ModÃ¨le LLM pour sÃ©lection ?** âœ… **GPT-5-mini** recommandÃ© (coÃ»t rÃ©duit, qualitÃ© suffisante pour sÃ©lection vs gÃ©nÃ©ration)
2. **Budget exploration ?** âœ… **100k tokens max** (modÃ¨le mini, coÃ»t trÃ¨s faible, sÃ©parÃ© de budget gÃ©nÃ©ration)
3. **Cache sÃ©lections ?** âœ… **TTL 24h** recommandÃ© (hash `user_instructions + sorted(hints) + expansion_radius + max_tokens_target`)
4. **Section filters granularitÃ© ?** âœ… **Sous-section pour MVP** (Phase 1), **Chunks ciblÃ©s pour Phase 2** (field_filters avec get_relation_chunks)
5. **IntÃ©gration embeddings ?** âœ… **BM25 suffit pour MVP** (vector search = V2.0 si recall insuffisant)

---

## Alternatives Considered

### Alternative 1 : SÃ©lection manuelle uniquement
- **RejetÃ©** : Trop cognitivement coÃ»teux, error-prone, ne rÃ©sout pas problÃ¨me dilution

### Alternative 2 : RAG classique (embedding + retrieval)
- **RejetÃ©** : Ne rÃ©sout pas granularitÃ© (fiche complÃ¨te vs sous-sections), pas de dÃ©ductions

### Alternative 3 : Agent libre (full RLM avec contexte prÃªt prompt)
- **RejetÃ©** : Bypasserait `build_context_json()`, casserait invariants, trop complexe

### Alternative 4 : Pipeline dÃ©terministe (rules-based selection)
- **ConsidÃ©rÃ©** : Plus simple, plus testable, mais moins flexible, ne rÃ©sout pas granularitÃ© fine

### Alternative 5 : GranularitÃ© paragraphe (chunks Ã  la volÃ©e)
- **ConsidÃ©rÃ©** : Trop complexe MVP, granularitÃ© sous-section + chunks ciblÃ©s (Phase 2) suffit pour rÃ©duction tokens

---

## Consequences

### Positives
- RÃ©duction significative friction utilisateur
- AmÃ©lioration recall (Ã©lÃ©ments pertinents non oubliÃ©s)
- RÃ©duction tokens contexte (20k+ â†’ 12-15k)
- CompatibilitÃ© avec architecture existante

### NÃ©gatives
- ComplexitÃ© ajoutÃ©e (nouveau service RLM)
- CoÃ»t LLM supplÃ©mentaire (exploration outillÃ©e)
- Latence ajoutÃ©e (sÃ©lection automatique)
- Non-dÃ©terminisme (mitigation : seed + cache)

---

## Implementation Recommendations

### Architecture

**Abstraction GDDToolsProvider:**
```python
# services/gdd_tools_provider.py (NOUVEAU)
class GDDToolsProvider:
    """Wrapper pour exposer outils GDD au LLM via function calling."""
    def __init__(self, element_repository: ElementRepository):
        self._repo = element_repository
    
    def get_relation_chunks(
        self,
        source_id: str,
        target_id: str,
        relation_field: str = "Relations"
    ) -> Dict[str, Any]:
        """Extrait uniquement les relations communes entre deux personnages.
        
        Exemple: UresaÃ¯r.Relations.Akthar (chunk commun) vs UresaÃ¯r.Relations complet.
        RÃ©duction tokens: 200 tokens vs 2000 tokens pour "Relations" complet.
        """
        # 1. RÃ©cupÃ©rer source_data et target_data via element_repository
        # 2. Extraire relations communes (intersection)
        # 3. Retourner chunks ciblÃ©s uniquement
```

**Limites RLM Agent:**
```python
class RLMContextSelector:
    MAX_TOOL_CALLS = 50  # Limite absolue appels outils (Ã©viter boucles infinies)
    MAX_EXPLORATION_TOKENS = 100000  # Budget max exploration (modÃ¨le mini, coÃ»t faible)
    
    async def select_context(...) -> ContextSelectionResult:
        tool_call_count = 0
        exploration_tokens = 0
        
        while tool_call_count < MAX_TOOL_CALLS:
            # ... LLM loop ...
            tool_call_count += 1
            exploration_tokens += estimated_tokens
            
            if exploration_tokens > MAX_EXPLORATION_TOKENS:
                logger.warning("Budget exploration dÃ©passÃ©, utilisation hints uniquement")
                return self._fallback_to_hints(hints)
```

### Phase Implementation

**Phase 1 (MVP):**
- RLM service avec granularitÃ© **sous-section** (`section_filters.include/exclude`)
- GDDToolsProvider abstraction (outils GDD pour LLM)
- Extension `ContextFieldManager.filter_fields_by_section_filters()`
- Router `/api/v1/context/select-context`
- Frontend toggle "Auto Selection"

**Phase 2 (AmÃ©lioration):**
- GranularitÃ© **chunks ciblÃ©s** (`field_filters` avec `get_relation_chunks()`)
- Validation sections existantes (warnings, pas erreurs)
- Cache sÃ©lections (TTL 24h)
- Tests E2E complets

### File Structure

**Backend:**
```
services/
  â”œâ”€â”€ rlm_context_selector.py        # NEW: RLM orchestration
  â”œâ”€â”€ gdd_tools_provider.py          # NEW: Outils GDD pour LLM
  â””â”€â”€ context_field_manager.py       # MODIFY: filter_fields_by_section_filters()

api/
  â”œâ”€â”€ routers/
  â”‚   â””â”€â”€ context.py                 # MODIFY: /select-context endpoint
  â””â”€â”€ schemas/
      â””â”€â”€ context.py                 # NEW: SelectContextRequest/Response
```

**Frontend:**
```
frontend/src/
  â”œâ”€â”€ components/generation/
  â”‚   â””â”€â”€ ContextSelector.tsx        # MODIFY: Toggle "Auto Selection"
  â”œâ”€â”€ store/
  â”‚   â””â”€â”€ generationStore.ts         # MODIFY: autoSelection state
  â””â”€â”€ api/
      â””â”€â”€ context.ts                 # MODIFY: selectContext() API call
```

**Tests:**
```
tests/
  â”œâ”€â”€ services/
  â”‚   â”œâ”€â”€ test_rlm_context_selector.py    # NEW
  â”‚   â””â”€â”€ test_gdd_tools_provider.py      # NEW
  â””â”€â”€ api/
      â””â”€â”€ test_context.py                 # MODIFY: /select-context tests
```

## References

- **Recursive Language Models (RLM)** - arXiv:2512.24601 - Alex L. Zhang, Tim Kraska, Omar Khattab
- **Context Rot** - Hong et al., 2025
- **Baleen** - Khattab et al., 2021 (retrieval-augmented generation)
