# V1.0 Architectural Decisions (ADRs)

### ADR-001: Progress Feedback Modal (Streaming LLM)

**Context:**  
UI "gel" pendant g√©n√©ration LLM (30s+), pas de feedback utilisateur ‚Üí UX critique bloquante

**Decision:**  
Modal centr√©e avec streaming SSE (Server-Sent Events)

**Technical Design:**

**Frontend:**
- Nouveau composant `GenerationProgressModal.tsx`
- State : Zustand slice `useGenerationStore` (√©tat streaming)
- API : EventSource SSE vers `/api/v1/generate/stream`
- UI : 2 zones (sortie LLM stream + √©tapes/logs), 2 actions (Interrompre/R√©duire)

**Backend:**
- Nouveau router `/api/v1/generate/stream` (SSE endpoint)
- Pattern : `async def` generator avec `yield` (chunks SSE)
- LLM : `stream=True` sur `responses.create()` (GPT-5.2)
- Format : `data: {"type": "chunk", "content": "..."}\n\n`

**Constraints:**
- **DOIT** utiliser Zustand (pattern existant state management)
- **DOIT** respecter format SSE (`data: ...\n\n`)
- **NE DOIT PAS** modifier panneau D√©tails (trop √©troit, modal n√©cessaire)
- **DOIT** g√©rer interruption propre (AbortController frontend + cleanup backend)

**Rationale:**
- SSE > WebSocket (unidirectionnel, plus simple, fallback HTTP)
- Modal > panneau int√©gr√© (340px insuffisant, focus utilisateur)
- Streaming natif GPT-5.2 Responses API (pas de polling)

**Risks:**
- SSE timeout long g√©n√©ration (mitigation : keep-alive pings)
- Gestion erreurs stream interrompu (mitigation : error events SSE)

**Tests Required:**
- Unit : `useGenerationStore` state transitions
- Integration : `/api/v1/generate/stream` SSE format
- E2E : Modal affichage + interruption mid-stream

**Acceptance Criteria:**
- [ ] Modal visible d√®s clic "G√©n√©rer"
- [ ] Sortie LLM stream√©e en temps r√©el (<500ms latency)
- [ ] Bouton "Interrompre" arr√™te g√©n√©ration proprement
- [ ] Fermeture modal restaure UI pr√©c√©dente

---

### ADR-002: Presets syst√®me (Configuration sauvegarde/chargement)

**Context:**  
Cold start friction : 10+ clics pour premier dialogue (s√©lection personnages, lieux, instructions)

**Decision:**  
Syst√®me presets avec sauvegarde/chargement configurations compl√®tes

**Technical Design:**

**Data Model:**
```typescript
interface Preset {
  id: string;
  name: string;
  icon: string; // emoji
  metadata: {
    created: Date;
    modified: Date;
  };
  configuration: {
    characters: string[];      // IDs s√©lectionn√©s
    locations: string[];
    region: string;
    subLocation?: string;
    sceneType: string;         // "Premi√®re rencontre", etc.
    instructions: string;      // Brief sc√®ne
  };
}
```

**Frontend:**
- Nouveau composant `PresetBar.tsx` (barre compacte au-dessus "Sc√®ne Principale")
- 2 boutons : "üìã Charger preset ‚ñº" (dropdown) + "üíæ Sauvegarder preset"
- Modal sauvegarde : nom, ic√¥ne emoji, aper√ßu lecture seule
- State : Zustand slice `usePresetStore`

**Backend:**
- Nouveau router `/api/v1/presets` (CRUD)
- Storage : Fichiers JSON locaux `data/presets/{preset_id}.json`
- Service : `PresetService` (validation, persistence)

**Constraints:**
- **DOIT** capturer configuration compl√®te (personnages + lieux + instructions)
- **DOIT** valider IDs r√©f√©rences (personnages/lieux existent dans GDD)
- **NE DOIT PAS** stocker contenu GDD (seulement IDs)
- **DOIT** g√©rer preset obsol√®te (r√©f√©rences GDD supprim√©es)

**Rationale:**
- Cold start ‚Üí 1 clic (objectif efficiency V1.0)
- Stockage local (pas besoin DB, Git-friendly)
- Validation lazy (au chargement, pas √† la sauvegarde)

**Risks:**
- GDD updates rendent presets obsol√®tes (mitigation : validation chargement + warning)
- Synchronisation multi-utilisateurs (hors scope MVP, V2.0 RBAC)

**Tests Required:**
- Unit : `PresetService` validation + persistence
- Integration : API `/api/v1/presets` CRUD
- E2E : Workflow complet sauvegarde ‚Üí chargement ‚Üí g√©n√©ration

**Acceptance Criteria:**
- [ ] Bouton "Sauvegarder preset" capture configuration actuelle
- [ ] Modal sauvegarde : nom + ic√¥ne + aper√ßu
- [ ] Dropdown "Charger preset" liste presets disponibles
- [ ] Chargement preset restaure configuration compl√®te
- [ ] Warning si r√©f√©rences GDD invalides

---

### ADR-003: Graph Editor Fixes (DisplayName vs stableID)

**Context:**  
Bug critique : DisplayName utilis√© comme ID au lieu de stableID ‚Üí corruption graphe

**Decision:**  
Correction imm√©diate + tests r√©gression

**Technical Design:**

**Root Cause:**
- React Flow utilise `node.id` comme identifiant unique
- Code actuel : `node.id = displayName` (peut changer, collisions)
- Correct : `node.id = stableID` (UUID immuable)

**Fix:**
```typescript
// Avant (BUGGY)
const node = {
  id: dialogue.displayName,  // ‚ùå Mutable, collisions
  data: { ... }
};

// Apr√®s (CORRECT)
const node = {
  id: dialogue.stableID,      // ‚úÖ UUID immuable
  data: { 
    displayName: dialogue.displayName,  // Affich√© dans UI
    ...
  }
};
```

**Impact Analysis:**
- Fichiers : `frontend/src/components/graph/GraphEditor.tsx`
- Composants : Node rendering, edge connections
- State : Zustand store `useGraphStore`

**Constraints:**
- **DOIT** migrer donn√©es existantes (stableID manquants ‚Üí g√©n√©ration UUID)
- **NE DOIT PAS** casser graphes existants (backward compatibility)
- **DOIT** ajouter tests r√©gression (collision displayName)

**Rationale:**
- Stabilit√© identifiants = graphe robuste
- S√©paration ID technique (UUID) vs display (nom √©ditable)

**Risks:**
- Migration donn√©es existantes (mitigation : script migration + backup)
- Edge cases (n≈ìuds sans stableID) (mitigation : g√©n√©ration UUID automatique)

**Tests Required:**
- Unit : `generateStableID()` unicit√©
- Integration : Graph serialization/deserialization
- E2E : Renommer dialogue ne casse pas connexions

**Acceptance Criteria:**
- [ ] `node.id` utilise `stableID` (UUID)
- [ ] Renommer dialogue preserve connexions
- [ ] Aucun graphe existant corrompu apr√®s migration
- [ ] Tests r√©gression collisions displayName

---

### ADR-004: Multi-Provider LLM Support (Mistral Small Creative)

**Context:**  
Actuellement, DialogueGenerator utilise uniquement OpenAI GPT-5.2. Besoin d'ajouter Mistral Small Creative comme alternative s√©lectionnable pour offrir plus de flexibilit√© et r√©duire la d√©pendance √† un seul provider.

**Decision:**  
Impl√©menter abstraction multi-provider avec support OpenAI (GPT-5.2) + Mistral (Small Creative) en V1.0. Utilisateur peut s√©lectionner le mod√®le via UI.

**Technical Design:**

**Backend Abstraction:**
- Interface `IGenerator` existante √©tendue pour supporter multiple providers
- Nouveau service `services/llm/mistral_client.py` impl√©mentant `IGenerator`
- Factory pattern : `LLMFactory.create(provider: str, model: str)` retourne client appropri√©
- Configuration : `config/llm_config.json` d√©finit providers disponibles + mod√®les

**Provider-Specific Implementations:**
- **OpenAI** : `OpenAIClient` (existant, Responses API GPT-5.2)
- **Mistral** : `MistralClient` (nouveau, Chat Completions API, Small Creative)
  - SDK : `mistralai` Python package
  - Streaming : Support natif via `stream=True`
  - Structured outputs : Via `response_format` (JSON Schema)

**Frontend Model Selection:**
- Nouveau composant `components/generation/ModelSelector.tsx` (dropdown)
- State : Zustand `generationStore.selectedModel` (provider + model)
- Options affich√©es : "OpenAI GPT-5.2", "Mistral Small Creative"
- Persistence : Pr√©f√©rence sauvegard√©e dans localStorage

**API Changes:**
- Endpoint `/api/v1/generate/stream` accepte param√®tre `model` (optionnel, d√©faut: OpenAI)
- Format : `?provider=openai&model=gpt-5.2` ou `?provider=mistral&model=small-creative`
- Backward compatible : Si `model` absent, utilise OpenAI (comportement actuel)

**Constraints:**
- **DOIT** maintenir backward compatibility (OpenAI reste d√©faut)
- **DOIT** utiliser abstraction `IGenerator` (pas de code provider-sp√©cifique dans routers)
- **DOIT** supporter streaming pour tous providers (SSE format identique)
- **DOIT** g√©rer structured outputs pour tous providers (JSON Schema)
- **NE DOIT PAS** exposer diff√©rences providers √† l'utilisateur (abstraction compl√®te)

**Rationale:**
- **Flexibilit√©** : Utilisateur choisit mod√®le selon besoins (qualit√© vs co√ªt vs vitesse)
- **R√©duction d√©pendance** : Pas de vendor lock-in, fallback si OpenAI down
- **Cost optimization** : Mistral Small Creative potentiellement moins cher
- **Abstraction propre** : Pattern IGenerator d√©j√† en place, extension naturelle

**Risks:**
- **Diff√©rences API** : OpenAI Responses API vs Mistral Chat Completions (mitigation : abstraction IGenerator)
- **Structured outputs** : Formats diff√©rents (mitigation : normalisation JSON Schema)
- **Streaming** : Impl√©mentations diff√©rentes (mitigation : wrapper uniforme SSE)
- **Cost tracking** : Prix diff√©rents par provider (mitigation : cost service multi-provider)

**Tests Required:**
- Unit : `MistralClient` impl√©mente `IGenerator` correctement
- Unit : `LLMFactory` retourne bon client selon provider
- Integration : `/api/v1/generate/stream?provider=mistral` fonctionne
- Integration : Streaming Mistral produit format SSE identique
- E2E : S√©lection mod√®le dans UI ‚Üí g√©n√©ration avec bon provider

**Acceptance Criteria:**
- [ ] Dropdown "Mod√®le" dans UI g√©n√©ration
- [ ] S√©lection Mistral Small Creative ‚Üí g√©n√©ration fonctionne
- [ ] Streaming SSE identique pour OpenAI et Mistral
- [ ] Structured outputs fonctionnent pour les deux providers
- [ ] Cost tracking diff√©renci√© par provider
- [ ] Pr√©f√©rence mod√®le persist√©e (localStorage)

---

### ADR-005: RLM Context Selector (Autonomous Context Selection)

**Context:**  
S√©lection manuelle de contexte GDD est **cognitivement co√ªteuse et error-prone** :
- Sc√®ne "minimale" (2 personnages + 1 lieu) fait d√©j√† **15-20k tokens** en mode full
- Utilisateur doit d√©cider manuellement quelles fiches inclure et en quel mode (full/excerpt)
- Risque d'oublier √©l√©ments pertinents (liens cosmologiques, factions, objets rituels)
- Granularit√© trop grossi√®re : fiche "full" = 6-8k tokens, m√™me si seule une section est pertinente

**Probl√®me fondamental :** Avec des contextes de 20k+ tokens, m√™me avec fen√™tres 128k, les effets de d√©gradation OOLONG apparaissent (attention dilu√©e, d√©pendances longues brouill√©es, rappel pr√©cis d√©grad√©). Le vrai probl√®me n'est pas "comment choisir quelles fiches charger" mais **"comment raisonner sur un univers dont la sc√®ne active p√®se d√©j√† 20k tokens"**.

**Decision:**  
Impl√©menter une **couche optionnelle (on/off) de LLM "s√©lecteur autonome de contexte"** inspir√©e du paradigme **Recursive Language Models (RLM)** (arXiv:2512.24601) :
- Le syst√®me devient l'agent de s√©lection (exploration programmatique + d√©ductions)
- L'utilisateur devient superviseur (valide/ajuste, avec mode override)
- R√©duction contextuelle intelligente : 20k+ tokens ‚Üí 12-15k tokens sans perte de pertinence

**Technical Design:**

**Phase 1. Context Selection (RLM Agent)**

**Service Backend:**
```python
# services/rlm_context_selector.py
class RLMContextSelector:
    async def select_context(
        self,
        user_instructions: str,  # Instructions de Sc√®ne
        hints: Optional[Dict[str, List[str]]] = None,  # Optionnel : verrouiller √©l√©ments
        hints_mode: Optional[Dict[str, str]] = None,  # {"character_A": "full"}
        exclude: Optional[List[str]] = None,  # IDs √† exclure
        expansion_radius: int = 1,  # 0=aucune, 1=graphe direct, 2=indirect
        max_tokens_target: int = 15000,  # Budget global
        seed: Optional[int] = None,  # Reproductibilit√©
    ) -> ContextSelectionResult:
        # 1. Parse user_instructions pour extraire entit√©s explicites
        # 2. Exploration outill√©e (search_bm25, get_related, get_snippet, etc.)
        # 3. D√©ductions (liens cosmologiques, factions, objets rituels, etc.)
        # 4. D√©cision full/excerpt + section_filters pour chaque fiche
        # 5. Budget check (si d√©passement, passer plus en excerpt ou exclure)
        # 6. Retourner selected_elements + justifications + trace
```

**Outils GDD (expos√©s au LLM via function calling):**
```python
# Outils de navigation JSON
- get_node(id) -> json
- get_fields(id, fields[]) -> json
- list_ids(type=None, where_field_exists=None, limit=...)
- schema_overview() -> stats + exemples

# Outils de recherche
- search_bm25(query, top_k=20, filter_type=None) -> [{id, score, snippet}]
- search_regex(pattern, field=None, top_k=20) -> matches
- search_by_key_value(key, value, exact=True)

# Outils d'extraction contr√¥l√©e
- get_snippet(id, field, max_chars=2000, around=None)
- get_related(id, relation_keys=[...], depth=1)

# Outils d'agr√©gation
- count(filter...)
- group_by(field, filter...)
- build_table(ids, columns) -> rows
- diff(id_a, id_b, fields)
```

**Output Phase 1:**
```python
{
  "selected_elements": {
    "characters": {
      "Uresa√Ør": {
        "mode": "full",
        "section_filters": {
          "include": ["Psychologie", "Arc.Actuel", "Relations.Akthar"],
          "exclude": ["R√¥le cosmologique complet", "Histoire compl√®te"],
          "reason": "Focus sur dynamique relationnelle et √©tat √©motionnel"
        },
        "justification": {
          "reason": "hint_explicit",
          "proof": None
        }
      },
      "Akthar": {
        "mode": "full",
        "section_filters": {
          "include": ["Psychologie", "Relations.Uresa√Ør", "Croyances"],
          "exclude": ["R√¥le cosmologique complet"]
        },
        "justification": {
          "reason": "hint_explicit",
          "proof": None
        }
      }
    },
    "locations": {
      "Nef Centrale": {
        "mode": "full",
        "section_filters": {...},
        "justification": {
          "reason": "mentioned_explicitly",
          "proof": "Sc√®ne se d√©roule dans la Nef Centrale"
        }
      },
      "L√©viathan P√©trifi√©": {
        "mode": "excerpt",
        "justification": {
          "reason": "deduction_context_cosmologique",
          "proof": "L√©viathan mentionn√© comme cadre cosmologique dans Uresa√Ør.sections.R√¥le",
          "search_trace": ["get_related('Uresa√Ør')", "search_by_key_value('type', 'lieu_cosmologique')"]
        }
      }
    }
  },
  "trace": {
    "tools_called": ["search_bm25", "get_related", "get_snippet", ...],
    "decisions": [...],
    "total_tokens_estimated": 12000  # Optimis√© vs 20k+ en manuel
  }
}
```

**Phase 2. Context Build (inchang√© mais enrichi)**

**Integration avec ContextFieldManager:**
```python
# services/context_field_manager.py
def filter_fields_by_section_filters(
    self,
    element_type: str,
    fields_to_include: List[str],
    section_filters: Optional[Dict[str, List[str]]] = None  # <-- NOUVEAU
) -> List[str]:
    # Combine r√®gles statiques (context_config.json) + r√®gles dynamiques (section_filters)
    # Sans bypasser le DSL de champs existant
```

**Backend API:**
```python
# api/routers/context.py
@router.post("/select-context", response_model=SelectContextResponse)
async def select_context_auto(
    request_data: SelectContextRequest,
    rlm_selector: Annotated[RLMContextSelector, Depends(get_rlm_context_selector)],
) -> SelectContextResponse:
    # Phase 1 : RLM s√©lection automatique
    selection_result = await rlm_selector.select_context(
        user_instructions=request_data.user_instructions,
        hints=request_data.hints,
        ...
    )
    # Phase 2 : build_context_json (inchang√©)
    structured_context = context_builder.build_context_json(
        selected_elements=selection_result.selected_elements,
        scene_instruction=request_data.user_instructions,
        ...
    )
    return SelectContextResponse(
        selected_elements=selection_result.selected_elements,
        context=structured_context,
        trace=selection_result.trace
    )
```

**Frontend UI:**
- Toggle "Auto Selection" (on/off) dans panneau contexte
- Affichage "Contexte auto-s√©lectionn√©" avec justifications cliquables
- Mode "Override" : utilisateur peut forcer/ajouter des √©l√©ments m√™me en auto
- Mode "Lock" : utilisateur peut verrouiller certains √©l√©ments (toujours inclus)

**Constraints:**
- **DOIT** √™tre optionnel (on/off), avec fallback vers s√©lection manuelle
- **DOIT** rester compatible avec `ContextFieldManager`, `ContextTruncator`, `ContextSerializer`
- **NE DOIT PAS** bypasser `build_context_json()` (Option A, pas Option B)
- **DOIT** produire `selected_elements` avec `section_filters` enrichis
- **DOIT** inclure `justification` et `trace` pour tra√ßabilit√©
- **DOIT** respecter hints explicites (toujours inclus, mode full par d√©faut)
- **DOIT** √™tre reproductible (seed optionnel) ou au minimum tra√ßable
- **DOIT** g√©rer fallback gracieux (si RLM √©choue, retourner hints uniquement, pas d'erreur)

**Rationale:**
- **R√©duction friction** : Plus besoin de s√©lection manuelle laborieuse (10+ clics ‚Üí 1 clic "Auto")
- **Am√©lioration recall** : RLM trouve √©l√©ments pertinents que l'utilisateur aurait oubli√©s
- **Granularit√© adaptative** : S√©lection fine de sous-sections (ex: Uresa√Ør 6k ‚Üí 2-3k tokens) sans perte pertinence
- **Paradigme RLM** : Navigation programmatique du GDD, lecture r√©cursive, m√©moire de travail compacte, agr√©gation progressive
- **Compatible existant** : S'int√®gre proprement avec `ContextBuilder` sans casser invariants

**Risks:**
- **Non-d√©terminisme** : Agent peut choisir trajectoire diff√©rente (mitigation : seed + cache + tra√ßabilit√©)
- **S√©lection inattendue** : RLM peut inclure √©l√©ments non souhait√©s (mitigation : override + lock + exclusions)
- **Co√ªt LLM** : Exploration outill√©e = plusieurs appels LLM (mitigation : budget s√©par√© + cache + mod√®le "cheap" pour s√©lection)
- **Latence** : S√©lection automatique ajoute d√©lai avant g√©n√©ration (mitigation : cache + streaming progress)
- **Tests** : Agent loop difficile √† tester sans fixtures synth√©tiques (mitigation : tests avec mini-GDD + mocks LLM)

**Tests Required:**
- Unit : `RLMContextSelector.select_context()` avec mocks LLM
- Unit : `ContextFieldManager.filter_fields_by_section_filters()` combine r√®gles
- Integration : `/api/v1/context/select-context` avec vrai LLM (tests co√ªteux, limiter)
- Integration : Fallback gracieux si RLM √©choue
- E2E : Workflow complet auto-selection ‚Üí build_context ‚Üí g√©n√©ration

**Acceptance Criteria:**
- [ ] Toggle "Auto Selection" dans UI contexte
- [ ] RLM produit `selected_elements` avec `section_filters`
- [ ] Phase 2 `build_context_json()` utilise `section_filters` correctement
- [ ] R√©duction tokens : 20k+ ‚Üí 12-15k sans perte pertinence
- [ ] Justifications affich√©es (utilisateur peut comprendre pourquoi √©l√©ment inclus)
- [ ] Mode override fonctionne (ajout/force √©l√©ments m√™me en auto)
- [ ] Fallback gracieux si RLM √©choue (pas d'erreur, retourne hints uniquement)
- [ ] Tra√ßabilit√© compl√®te (trace contient trajectoire agent)

**Open Questions:**
- Mod√®le LLM pour s√©lection ? (GPT-5-mini pour co√ªt vs GPT-5.2 pour qualit√©)
- Budget exploration ? (5-10k tokens max pour phase 1 vs budget global g√©n√©ration)
- Cache s√©lections ? (m√™me `user_instructions` + `hints` = r√©sultat identique)
- Section filters granularit√© ? (niveau champ vs niveau sous-section vs niveau paragraphe)

---

### Integration Patterns (V1.0 ‚Üî Baseline)

#### Pattern 1: New API Endpoints (Streaming, Presets)

**Integration:**
- Nouveau router dans `api/routers/` (ex: `streaming.py`, `presets.py`)
- Enregistrement dans `api/main.py` : `app.include_router(streaming_router)`
- Service backend dans `services/` si logique m√©tier (ex: `PresetService`)
- Tests dans `tests/api/test_<router>.py`

**Follows Baseline:**
- ‚úÖ RESTful conventions (`/api/v1/*`)
- ‚úÖ Pydantic schemas (`api/schemas/`)
- ‚úÖ Dependency injection (`api/dependencies.py`)
- ‚úÖ Error handling (exceptions hi√©rarchis√©es)

#### Pattern 2: New React Components (Modal, PresetBar)

**Integration:**
- Nouveaux composants dans `frontend/src/components/<domain>/`
- State management via Zustand (nouveaux slices si n√©cessaire)
- API calls via `frontend/src/api/<domain>.ts`
- Tests dans `frontend/src/components/<domain>/<Component>.test.tsx`

**Follows Baseline:**
- ‚úÖ TypeScript strict
- ‚úÖ Zustand pour state global
- ‚úÖ API client modulaire (axios + intercepteurs)
- ‚úÖ Tests unitaires (Vitest + RTL)

#### Pattern 3: Graph Editor Fixes (Refactoring)

**Integration:**
- Modifications dans `frontend/src/components/graph/`
- Migration donn√©es si n√©cessaire (script `scripts/migrate-stableIDs.ts`)
- Tests r√©gression dans `frontend/src/components/graph/GraphEditor.test.tsx`

**Follows Baseline:**
- ‚úÖ Pas de breaking changes API
- ‚úÖ Backward compatibility (migrations gracieuses)
- ‚úÖ Tests couvrent edge cases

---

