### Epic 15: RLM Context Selector (S√©lection Automatique Contexte GDD)

**CONTEXTE CRITIQUE** : La s√©lection manuelle de sous-sections et sous-parties de fiches GDD est cognitivement co√ªteuse et error-prone. Les contextes de 20k+ tokens causent "context rot" (d√©gradation attention, d√©pendances longues brouill√©es, rappel pr√©cis d√©grad√©) observ√© en test. Cette Epic impl√©mente un service optionnel RLM (Recursive Language Models) qui explore programmatiquement le GDD via function calling, s√©lectionne intelligemment les √©l√©ments pertinents (fiches, sous-sections, sous-parties), et r√©duit le contexte de 20k+ ‚Üí 12-15k tokens (Phase 1) ‚Üí 6-10k tokens (Phase 2) sans perte de pertinence.

Les utilisateurs peuvent activer/d√©sactiver la s√©lection automatique via toggle "Auto Selection" (√† gauche du bouton "G√©n√©rer", panneau de droite), voir les justifications des s√©lections (format compact, d√©tails on-demand), et utiliser override/lock pour forcer/ajouter des √©l√©ments m√™me en auto. Le service est optionnel (fallback s√©lection manuelle si d√©sactiv√©) et s'int√®gre avec `ContextBuilder` sans casser invariants.

**FRs covered:** FR1-FR8 (RLM Context Selector - Toggle UI, Service RLM, Outils GDD, Extension ContextFieldManager, Endpoint API, Affichage Justifications, Mode Override, Fallback Gracieux)

**NFRs covered:** NFR1-NFR6 (Performance <5s latence, Usability justifications claires, Reliability fallback gracieux, Testability mocks LLM, Reproducibility seed optionnel, Compatibility ContextBuilder pr√©serv√©)

**Valeur utilisateur:** R√©duction friction s√©lection contexte (10+ clics ‚Üí 1 clic), r√©duction tokens contexte (20k+ ‚Üí 12-15k Phase 1, 6-10k Phase 2) sans perte pertinence, expliquabilit√© (justifications claires), contr√¥le utilisateur (override/lock).

**D√©pendances:** Aucune (service optionnel, peut √™tre d√©velopp√© ind√©pendamment). Compatible Epic 3 (Gestion contexte narratif GDD) mais ne bloque pas.

**Implementation Priority:** Epic 15 Story 1 = Service RLM `RLMContextSelector` + Outils GDD `GDDToolsProvider` - **FONDATION** pour toutes les autres stories

**Related ADR:** ADR-005 (RLM Context Selector - Autonomous Context Selection)

---

## ‚ö†Ô∏è GARDE-FOUS - V√©rification de l'Existant (Scrum Master)

**OBLIGATOIRE avant cr√©ation de chaque story de cet epic :**

### Checklist de V√©rification

1. **Fichiers mentionn√©s dans les stories :**
   - [ ] V√©rifier existence avec `glob_file_search` ou `grep`
   - [ ] V√©rifier chemins corrects (ex: `services/` vs `core/context/`)
   - [ ] Si existe : **D√âCISION** - √âtendre ou remplacer ? (documenter dans story)

2. **Composants/Services similaires :**
   - [ ] Rechercher composants React similaires (`codebase_search` dans `frontend/src/components/`)
   - [ ] Rechercher stores Zustand similaires (`codebase_search` dans `frontend/src/store/`)
   - [ ] Rechercher services Python similaires (`codebase_search` dans `services/`, `core/context/`)
   - [ ] Si similaire existe : **D√âCISION** - R√©utiliser ou cr√©er nouveau ? (documenter dans story)

3. **Endpoints API :**
   - [ ] V√©rifier namespace coh√©rent (`/api/v1/context/*` vs `/api/v1/dialogues/*`)
   - [ ] V√©rifier si endpoint similaire existe (`grep` dans `api/routers/`)
   - [ ] Si endpoint similaire : **D√âCISION** - √âtendre ou cr√©er nouveau ? (documenter dans story)

4. **Patterns existants :**
   - [ ] V√©rifier patterns Zustand (immutable updates, structure stores)
   - [ ] V√©rifier patterns FastAPI (routers, dependencies, schemas)
   - [ ] V√©rifier patterns React (composants, hooks, toggles)
   - [ ] Respecter conventions de nommage et structure dossiers

5. **Documentation des d√©cisions :**
   - Si remplacement : Documenter **POURQUOI** dans story "Dev Notes"
   - Si extension : Documenter **COMMENT** (quels champs/m√©thodes ajouter)
   - Si nouveau : Documenter **POURQUOI** pas de r√©utilisation

### Fichiers/Composants Sp√©cifiques Epic 15

**OBLIGATOIRE avant cr√©ation de chaque story de cet epic :**

### Checklist Sp√©cifique Epic 15

1. **Fichiers existants √† v√©rifier :**
   - [ ] `core/context/context_builder.py` (existe d√©j√†, g√®re build_context_json)
   - [ ] `services/context_field_manager.py` (existe d√©j√†, g√®re filtrage champs)
   - [ ] `frontend/src/store/generationStore.ts` (existe d√©j√†, g√®re sceneSelection)
   - [ ] `api/routers/context.py` ou similaire (v√©rifier existence)
   - [ ] `config/context_config.json` (existe d√©j√†, r√®gles statiques champs)

2. **Patterns existants √† respecter :**
   - [ ] Zustand stores : Immutable updates, pattern `set((state) => ({ ...state, newValue }))`
   - [ ] FastAPI routers : Namespace `/api/v1/context/*` (coh√©rent avec autres routers)
   - [ ] React toggles : Pattern existant (v√©rifier composants similaires)
   - [ ] Context builder : Ne pas bypasser `build_context_json()`, utiliser section_filters

3. **D√©cisions de remplacement :**
   - Si story propose de cr√©er un fichier qui existe : **DOCUMENTER** d√©cision (√©tendre vs remplacer)
   - Si story propose un chemin incorrect : **CORRIGER** avant cr√©ation
   - Si story propose un pattern diff√©rent : **JUSTIFIER** dans "Dev Notes"

---

### Story 15.1: Service RLM ContextSelector + Outils GDD GDDToolsProvider (Fondation)

As a **utilisateur g√©n√©rant des dialogues**,
I want **qu'un agent LLM explore automatiquement le GDD et s√©lectionne intelligemment les √©l√©ments pertinents (fiches, sous-sections)**,
So that **je r√©duis la friction de s√©lection manuelle et j'obtiens un contexte optimis√© (20k+ ‚Üí 12-15k tokens) sans perte de pertinence**.

**Acceptance Criteria:**

**Given** un service `RLMContextSelector` existe
**When** j'appelle `select_context(user_instructions, hints, hints_mode, exclude, expansion_radius, max_tokens_target, seed)`
**Then** le service explore le GDD via outils `GDDToolsProvider` (search_bm25, get_related, get_snippet, etc.)
**And** le service produit `ContextSelectionResult` avec `selected_elements` (fiches + modes + `section_filters`), `justifications` (raison + preuve), `trace` (outils appel√©s, d√©cisions)

**Given** le service respecte limites `MAX_TOOL_CALLS = 50` et `MAX_EXPLORATION_TOKENS = 100000`
**When** une exploration d√©passe ces limites
**Then** le service retourne fallback gracieux (hints uniquement, pas d'erreur)
**And** un warning est logg√© ("Budget exploration d√©pass√©, utilisation hints uniquement")

**Given** le service re√ßoit hints explicites (personnages/lieux mentionn√©s)
**When** le service s√©lectionne les √©l√©ments
**Then** les hints sont toujours inclus (mode full par d√©faut)
**And** les hints sont marqu√©s `justification.reason = "hint_explicit"`

**Given** un service `GDDToolsProvider` existe
**When** le service expose outils GDD au LLM via function calling
**Then** les outils suivants sont disponibles : `get_node(id)`, `get_fields(id, fields[])`, `list_ids(type, where_field_exists, limit)`, `schema_overview()`, `search_bm25(query, top_k, filter_type)`, `search_regex(pattern, field, top_k)`, `search_by_key_value(key, value, exact)`, `get_snippet(id, field, max_chars, around)`, `get_related(id, relation_keys, depth)`, `count(filter)`, `group_by(field, filter)`, `build_table(ids, columns)`, `diff(id_a, id_b, fields)`

**Given** le service utilise mod√®le GPT-5-mini pour s√©lection (co√ªt r√©duit)
**When** le service explore le GDD
**Then** le budget exploration est s√©par√© du budget g√©n√©ration (100k tokens max, mod√®le mini)
**And** les co√ªts exploration sont track√©s s√©par√©ment

**Technical Requirements:**
- Backend : Cr√©er `services/rlm_context_selector.py` avec classe `RLMContextSelector`
  - M√©thode `async def select_context(...) -> ContextSelectionResult`
  - Impl√©mentation paradigme RLM : Navigation programmatique GDD, lecture r√©cursive, m√©moire de travail compacte, agr√©gation progressive
  - Limites : `MAX_TOOL_CALLS = 50`, `MAX_EXPLORATION_TOKENS = 100000`
  - Fallback : Si limites d√©pass√©es ou erreur LLM ‚Üí retourner hints uniquement (pas d'erreur)
  - Mod√®le LLM : GPT-5-mini (co√ªt r√©duit, qualit√© suffisante pour s√©lection)
  - Seed optionnel : Pour reproductibilit√© (m√™me s√©lection pour m√™mes inputs)
- Backend : Cr√©er `services/gdd_tools_provider.py` avec classe `GDDToolsProvider`
  - Abstraction pour exposer outils GDD au LLM via function calling
  - Outils disponibles : `get_node`, `get_fields`, `list_ids`, `schema_overview`, `search_bm25`, `search_regex`, `search_by_key_value`, `get_snippet`, `get_related`, `count`, `group_by`, `build_table`, `diff`
  - Injection d√©pendances : `ElementRepository` pour acc√®s GDD
- Backend : Sch√©mas Pydantic `api/schemas/context.py`
  - `SelectContextRequest` (user_instructions, hints, hints_mode, exclude, expansion_radius, max_tokens_target, seed)
  - `SelectContextResponse` (selected_elements, context, trace)
  - `ContextSelectionResult` (selected_elements avec section_filters, justifications, trace)
- Tests : Unit (RLMContextSelector avec mocks LLM, GDDToolsProvider avec mocks repository), Integration (exploration GDD r√©el avec mini-GDD)

**Dev Notes:**

**Architecture Patterns:**
- **Paradigme RLM** : Le service utilise Recursive Language Models (arXiv:2512.24601) - navigation programmatique GDD, lecture r√©cursive, m√©moire de travail compacte, agr√©gation progressive
- **Function Calling** : `GDDToolsProvider` expose outils GDD au LLM via function calling (OpenAI tools/tool_choice)
- **Limites s√©curit√©** : `MAX_TOOL_CALLS = 50` et `MAX_EXPLORATION_TOKENS = 100000` pour √©viter boucles infinies et co√ªts excessifs
- **Fallback gracieux** : Si RLM √©choue (budget d√©pass√©, erreur LLM, limite tool calls), retourner hints uniquement (pas d'erreur, s√©lection manuelle pr√©serv√©e)

**R√©utilisation Code:**
- **ElementRepository** : `GDDToolsProvider` utilise `ElementRepository` existant pour acc√®s GDD (pas de duplication)
- **LLM Client** : `RLMContextSelector` utilise `ILLMClient` existant (via factory) pour appels LLM
- **Mod√®le GPT-5-mini** : Utiliser mod√®le mini pour s√©lection (co√ªt r√©duit, qualit√© suffisante vs g√©n√©ration GPT-5.2)

**References:** ADR-005 (RLM Context Selector), FR2-FR3 (Service RLM, Outils GDD), NFR1 (Performance <5s), NFR3 (Reliability fallback gracieux), NFR4 (Testability mocks LLM)

---

### Story 15.2: Extension ContextFieldManager avec section_filters

As a **syst√®me de g√©n√©ration de dialogues**,
I want **que `ContextFieldManager` puisse filtrer les champs par sous-sections (include/exclude)**,
So that **les s√©lections RLM (section_filters) sont correctement appliqu√©es lors du build_context_json()**.

**Acceptance Criteria:**

**Given** une m√©thode `ContextFieldManager.filter_fields_by_section_filters()` existe
**When** j'appelle la m√©thode avec `section_filters = {"include": ["Relations.Akthar"], "exclude": ["R√¥le cosmologique"]}`
**Then** la m√©thode combine r√®gles statiques (`context_config.json`) + r√®gles dynamiques (`section_filters`)
**And** seules les sous-sections incluses sont conserv√©es (exclusions appliqu√©es)
**And** le DSL de champs existant n'est pas bypass√© (r√®gles statiques pr√©serv√©es)

**Given** `section_filters` contient `include` (sous-sections √† inclure)
**When** la m√©thode filtre les champs
**Then** uniquement les sous-sections list√©es dans `include` sont conserv√©es
**And** les autres sous-sections sont exclues (sauf si dans r√®gles statiques obligatoires)

**Given** `section_filters` contient `exclude` (sous-sections √† exclure)
**When** la m√©thode filtre les champs
**Then** les sous-sections list√©es dans `exclude` sont supprim√©es
**And** les autres sous-sections sont conserv√©es (selon r√®gles statiques)

**Given** `section_filters` est `None` ou vide
**When** la m√©thode filtre les champs
**Then** seule la logique statique (`context_config.json`) est appliqu√©e (pas de changement comportement existant)
**And** aucun filtrage dynamique n'est appliqu√©

**Technical Requirements:**
- Backend : √âtendre `services/context_field_manager.py` avec m√©thode `filter_fields_by_section_filters(element_type, fields_to_include, section_filters)`
  - Param√®tre `section_filters: Optional[Dict[str, Any]]` avec `include` (list sous-sections), `exclude` (list sous-sections)
  - Combiner r√®gles statiques (`context_config.json`) + r√®gles dynamiques (`section_filters`)
  - Ne pas bypasser le DSL de champs existant (r√®gles statiques pr√©serv√©es)
  - Retourner `List[str]` champs filtr√©s
- Backend : Int√©gration avec `ContextBuilder.build_context_json()`
  - Passer `section_filters` depuis `ContextSelectionResult` √† `ContextFieldManager`
  - Appliquer filtrage lors de l'extraction des champs pour chaque √©l√©ment
- Tests : Unit (filter_fields_by_section_filters avec include/exclude/None), Integration (build_context_json avec section_filters)

**Dev Notes:**

**Architecture Patterns:**
- **Extension vs Remplacement** : √âtendre `ContextFieldManager` existant (ne pas cr√©er nouveau service)
- **Compatibilit√©** : Si `section_filters` est `None`, comportement identique √† avant (pas de breaking change)
- **DSL pr√©serv√©** : Les r√®gles statiques `context_config.json` sont toujours appliqu√©es en premier, puis `section_filters` applique filtrage suppl√©mentaire

**R√©utilisation Code:**
- **ContextFieldManager** : √âtendre classe existante (pas de duplication)
- **context_config.json** : Continuer √† utiliser r√®gles statiques existantes (pas de migration)

**References:** ADR-005 (RLM Context Selector - Phase 2 Context Build), FR4 (Extension ContextFieldManager), NFR6 (Compatibility ContextBuilder pr√©serv√©)

---

### Story 15.3: Endpoint API /select-context

As a **frontend React**,
I want **un endpoint API `/api/v1/context/select-context` qui ex√©cute s√©lection automatique RLM puis build_context_json()**,
So that **je peux obtenir un contexte optimis√© avec s√©lection automatique en un seul appel**.

**Acceptance Criteria:**

**Given** un endpoint POST `/api/v1/context/select-context` existe
**When** j'envoie `SelectContextRequest` (user_instructions, hints, hints_mode, exclude, expansion_radius, max_tokens_target, seed)
**Then** l'endpoint ex√©cute Phase 1 (RLM s√©lection automatique via `RLMContextSelector.select_context()`)
**And** l'endpoint ex√©cute Phase 2 (`build_context_json()` avec `selected_elements` et `section_filters`)
**And** l'endpoint retourne `SelectContextResponse` (selected_elements, context, trace)

**Given** le RLM √©choue (budget d√©pass√©, erreur LLM, limite tool calls)
**When** l'endpoint re√ßoit erreur RLM
**Then** l'endpoint retourne fallback gracieux (hints uniquement, `context` construit avec hints, pas d'erreur HTTP)
**And** le code HTTP est 200 (succ√®s avec fallback, pas d'erreur 500)

**Given** l'endpoint re√ßoit une requ√™te invalide (user_instructions manquant)
**When** la validation Pydantic √©choue
**Then** l'endpoint retourne erreur 422 (Validation Error) avec d√©tails champs manquants

**Given** l'endpoint s'int√®gre avec `ContextBuilder`
**When** l'endpoint appelle `build_context_json()`
**Then** `build_context_json()` utilise `selected_elements` et `section_filters` depuis `ContextSelectionResult`
**And** `build_context_json()` ne bypass pas l'invariant existant (logique pr√©serv√©e)

**Technical Requirements:**
- Backend : Cr√©er ou √©tendre `api/routers/context.py` avec endpoint POST `/api/v1/context/select-context`
  - D√©pendances : `RLMContextSelector` (via `Depends(get_rlm_context_selector)`), `ContextBuilder` (via `Depends(get_context_builder)`)
  - Validation : `SelectContextRequest` (Pydantic schema)
  - Phase 1 : Appeler `rlm_selector.select_context(...)`
  - Phase 2 : Appeler `context_builder.build_context_json(selected_elements, scene_instruction, section_filters)`
  - R√©ponse : `SelectContextResponse` (Pydantic schema)
  - Gestion erreurs : Fallback gracieux si RLM √©choue (hints uniquement, HTTP 200)
- Backend : Injection d√©pendances `api/dependencies.py`
  - Fonction `get_rlm_context_selector() -> RLMContextSelector`
  - Fonction `get_context_builder() -> ContextBuilder` (existe d√©j√† ou cr√©er)
- Tests : Integration (endpoint avec mocks RLM + ContextBuilder), E2E (endpoint avec vrai RLM mini-GDD)

**Dev Notes:**

**Architecture Patterns:**
- **Namespace coh√©rent** : `/api/v1/context/*` (coh√©rent avec autres routers `/api/v1/dialogues/*`)
- **Two-Phase Execution** : Phase 1 (RLM s√©lection) ‚Üí Phase 2 (build_context_json) - pas de bypass
- **Fallback gracieux** : HTTP 200 avec fallback (pas d'erreur 500) ‚Üí service optionnel, pas de casse si RLM indisponible

**R√©utilisation Code:**
- **ContextBuilder** : R√©utiliser `ContextBuilder` existant (pas de duplication)
- **Dependencies** : Utiliser `api/dependencies.py` pour injection (pattern FastAPI existant)

**References:** ADR-005 (RLM Context Selector - Backend API), FR5 (Endpoint API /select-context), NFR6 (Compatibility ContextBuilder pr√©serv√©)

---

### Story 15.4: Toggle Auto Selection UI + Affichage Justifications

As a **utilisateur g√©n√©rant des dialogues**,
I want **un toggle "Auto Selection" dans le panneau de droite (√† gauche du bouton "G√©n√©rer") et voir les justifications des s√©lections automatiques**,
So that **je peux activer/d√©sactiver la s√©lection automatique et comprendre pourquoi les √©l√©ments sont s√©lectionn√©s**.

**Acceptance Criteria:**

**Given** je suis sur l'√©cran de g√©n√©ration (panneau de droite)
**When** je regarde le panneau de contexte
**Then** je vois un toggle "Auto Selection" positionn√© √† gauche du bouton "G√©n√©rer"
**And** le toggle est d√©sactiv√© par d√©faut (s√©lection manuelle pr√©serv√©e)

**Given** je active le toggle "Auto Selection"
**When** je lance une s√©lection de contexte
**Then** l'API `/api/v1/context/select-context` est appel√©e avec mes `user_instructions` et `hints`
**And** un indicateur de progression s'affiche ("Exploration du GDD... 2/50 appels outils")
**And** apr√®s s√©lection, les justifications s'affichent dans le panneau contexte (format compact)

**Given** les justifications sont affich√©es (format compact par d√©faut)
**When** je clique sur une justification
**Then** les d√©tails s'affichent on-demand (raison + preuve + trace exploratoire)
**And** les justifications incluent ic√¥ne visuelle du type de raison (hint_explicit, deduction_context_cosmologique, mentioned_explicitly, etc.)

**Given** je d√©sactive le toggle "Auto Selection"
**When** je lance une s√©lection de contexte
**Then** la s√©lection manuelle existante est utilis√©e (comportement inchang√©)
**And** aucune API RLM n'est appel√©e

**Technical Requirements:**
- Frontend : Composant `frontend/src/components/generation/ContextSelector.tsx` (modifier ou cr√©er)
  - Ajouter toggle "Auto Selection" √† gauche du bouton "G√©n√©rer"
  - Toggle synchronis√© avec `generationStore.autoSelection` (boolean)
  - Affichage justifications : Format compact par d√©faut, d√©tails on-demand au clic
  - Ic√¥nes visuelles : Type de raison (hint_explicit ‚Üí ‚úÖ, deduction_context_cosmologique ‚Üí üîç, mentioned_explicitly ‚Üí üìù, etc.)
- Frontend : Zustand store `frontend/src/store/generationStore.ts` (√©tendre)
  - √âtat `autoSelection: boolean` (d√©faut false)
  - Action `setAutoSelection(enabled: boolean)`
  - Action `selectContextAuto(request: SelectContextRequest)` pour appeler API `/api/v1/context/select-context`
- Frontend : API call `frontend/src/api/context.ts` (cr√©er ou √©tendre)
  - Fonction `selectContextAuto(request: SelectContextRequest): Promise<SelectContextResponse>`
  - Appel POST `/api/v1/context/select-context`
  - Gestion erreurs : Fallback gracieux (afficher message non-bloquant si RLM indisponible)
- Frontend : Indicateur progression pendant s√©lection RLM
  - Affichage "Exploration du GDD... X/50 appels outils" (mise √† jour temps r√©el via polling ou SSE si disponible)
- Tests : Unit (toggle synchronis√© avec store), Integration (API call fonctionne), E2E (toggle activ√© ‚Üí s√©lection auto ‚Üí justifications affich√©es)

**Dev Notes:**

**Architecture Patterns:**
- **Toggle position** : √Ä gauche du bouton "G√©n√©rer" (panneau de droite) - sp√©cification utilisateur
- **Format compact** : Justifications par d√©faut (pas de surcharge visuelle), d√©tails on-demand (click pour expand)
- **Ic√¥nes visuelles** : Type de raison avec ic√¥ne distincte (UX claire)

**R√©utilisation Code:**
- **generationStore** : √âtendre store existant (pas de cr√©er nouveau)
- **ContextSelector** : Modifier composant existant ou cr√©er nouveau selon architecture actuelle

**References:** ADR-005 (RLM Context Selector - Frontend UI), FR1 (Toggle Auto Selection UI), FR6 (Affichage Justifications UI), NFR2 (Usability justifications claires)

---

### Story 15.5: Mode Override + Lock

As a **utilisateur g√©n√©rant des dialogues**,
I want **pouvoir forcer/ajouter des √©l√©ments m√™me en mode auto-selection et verrouiller des √©l√©ments critiques**,
So that **je garde le contr√¥le sur les s√©lections automatiques et je peux ajuster selon mes besoins**.

**Acceptance Criteria:**

**Given** j'ai activ√© le toggle "Auto Selection"
**When** la s√©lection automatique propose une s√©lection
**Then** je peux ajouter un √©l√©ment manquant via mode override (bouton "Ajouter √©l√©ment" ou similaire)
**And** l'√©l√©ment ajout√© est marqu√© comme "override" (inclus m√™me si non s√©lectionn√© par RLM)

**Given** j'ai ajout√© un √©l√©ment via override
**When** je relance une s√©lection automatique (m√™me `user_instructions`)
**Then** l'√©l√©ment override est toujours inclus (pr√©serv√© entre s√©lections)
**And** le RLM peut toujours s√©lectionner d'autres √©l√©ments (override n'emp√™che pas RLM)

**Given** un √©l√©ment est critique (mentionn√© fr√©quemment)
**When** je verrouille l'√©l√©ment (lock)
**Then** l'√©l√©ment est toujours inclus dans toutes les s√©lections futures (m√™me si toggle d√©sactiv√© puis r√©activ√©)
**And** l'√©l√©ment est marqu√© visuellement comme "verrouill√©" (ic√¥ne üîí)

**Given** je d√©sactive le toggle "Auto Selection"
**When** je r√©active le toggle
**Then** les √©l√©ments verrouill√©s (lock) sont toujours inclus
**And** les √©l√©ments override sont pr√©serv√©s (si toujours pertinents)

**Given** je d√©verrouille un √©l√©ment (unlock)
**When** je relance une s√©lection automatique
**Then** l'√©l√©ment n'est plus forc√© inclus (RLM peut d√©cider de l'inclure ou non)

**Technical Requirements:**
- Frontend : Extension `ContextSelector.tsx`
  - Bouton "Ajouter √©l√©ment" en mode override (ajouter √©l√©ment √† s√©lection auto)
  - Bouton "Verrouiller" (lock) pour chaque √©l√©ment s√©lectionn√© (forcer inclusion permanente)
  - Bouton "D√©verrouiller" (unlock) pour √©l√©ments verrouill√©s
  - Affichage visuel : Ic√¥ne üîí pour √©l√©ments verrouill√©s, badge "Override" pour √©l√©ments ajout√©s manuellement
- Frontend : Zustand store `generationStore.ts` (√©tendre)
  - √âtat `lockedElements: string[]` (IDs √©l√©ments verrouill√©s)
  - √âtat `overrideElements: string[]` (IDs √©l√©ments ajout√©s via override)
  - Action `lockElement(id: string)`
  - Action `unlockElement(id: string)`
  - Action `addOverrideElement(id: string)`
  - Action `removeOverrideElement(id: string)`
- Frontend : Int√©gration avec API `/api/v1/context/select-context`
  - Passer `lockedElements` et `overrideElements` dans `SelectContextRequest`
  - Backend doit toujours inclure locked/override elements (m√™me si non s√©lectionn√©s par RLM)
- Backend : Extension `RLMContextSelector.select_context()`
  - Param√®tre `locked_elements: Optional[List[str]]` (IDs √©l√©ments verrouill√©s)
  - Param√®tre `override_elements: Optional[List[str]]` (IDs √©l√©ments ajout√©s via override)
  - Toujours inclure locked/override elements dans `selected_elements` (mode full par d√©faut)
- Tests : Unit (lock/unlock/override state), Integration (API avec locked/override), E2E (verrouiller √©l√©ment ‚Üí toujours inclus)

**Dev Notes:**

**Architecture Patterns:**
- **Override vs Lock** : Override = ajout temporaire pour s√©lection actuelle, Lock = inclusion permanente pour toutes s√©lections futures
- **Persistance** : Locked elements persist√©s dans localStorage (ou backend si disponible) pour survie entre sessions
- **Backend respect** : RLM doit toujours inclure locked/override elements (contrainte forte, pas option)

**R√©utilisation Code:**
- **generationStore** : √âtendre store existant (pas de cr√©er nouveau)
- **ContextSelector** : Modifier composant existant

**References:** ADR-005 (RLM Context Selector - Mode Override), FR7 (Mode Override), NFR2 (Usability contr√¥le utilisateur)

---

### Story 15.6: Fallback Gracieux + Tests E2E

As a **utilisateur g√©n√©rant des dialogues**,
I want **que le syst√®me g√®re gracieusement les √©checs RLM (budget d√©pass√©, erreur LLM, limite tool calls)**,
So that **je peux continuer √† travailler m√™me si la s√©lection automatique est indisponible**.

**Acceptance Criteria:**

**Given** le RLM √©choue (budget exploration d√©pass√©, erreur LLM, limite tool calls atteinte)
**When** l'API `/api/v1/context/select-context` est appel√©e
**Then** l'API retourne HTTP 200 (succ√®s avec fallback, pas d'erreur 500)
**And** la r√©ponse contient `selected_elements` avec hints uniquement (pas d'erreur)
**And** un message non-bloquant s'affiche "S√©lection automatique indisponible, utilisation hints uniquement"

**Given** le RLM √©choue avec erreur r√©seau (timeout, connexion perdue)
**When** l'API est appel√©e
**Then** l'API retourne fallback gracieux (hints uniquement, HTTP 200)
**And** un message s'affiche "Erreur r√©seau, utilisation hints uniquement"
**And** l'utilisateur peut r√©essayer ou basculer vers s√©lection manuelle

**Given** le RLM √©choue mais l'utilisateur a des hints explicites
**When** le fallback gracieux est activ√©
**Then** les hints sont utilis√©s pour construire `context` (via `build_context_json()`)
**And** le `context` g√©n√©r√© est utilisable (pas de contexte vide)
**And** l'utilisateur peut lancer une g√©n√©ration normale

**Given** le RLM √©choue et l'utilisateur n'a pas de hints
**When** le fallback gracieux est activ√©
**Then** le `context` retourn√© est minimal (structure vide mais valide)
**And** un message s'affiche "Aucune s√©lection disponible, veuillez ajouter des hints ou utiliser s√©lection manuelle"
**And** l'utilisateur peut ajouter hints ou basculer vers s√©lection manuelle

**Technical Requirements:**
- Backend : Extension `RLMContextSelector.select_context()`
  - Try/except autour de toute la logique RLM
  - Si exception (budget d√©pass√©, erreur LLM, limite tool calls) ‚Üí retourner `ContextSelectionResult` avec hints uniquement (pas d'erreur)
  - Logging : Logger warning avec d√©tails erreur (pour debug, pas expos√© √† utilisateur)
- Backend : Extension endpoint `/api/v1/context/select-context`
  - Try/except autour de `rlm_selector.select_context()`
  - Si exception ‚Üí construire fallback `ContextSelectionResult` avec hints uniquement
  - Toujours retourner HTTP 200 (succ√®s avec fallback)
  - Message fallback dans `trace` ("RLM unavailable, using hints only")
- Frontend : Gestion erreurs `ContextSelector.tsx`
  - Afficher message non-bloquant si fallback activ√© (toast ou banner, pas modal bloquante)
  - Permettre utilisateur de continuer (pas de blocage)
- Tests : Unit (RLM √©choue ‚Üí fallback gracieux), Integration (API avec erreur RLM ‚Üí HTTP 200 fallback), E2E (workflow complet auto-selection ‚Üí build_context ‚Üí g√©n√©ration avec fallback)

**Dev Notes:**

**Architecture Patterns:**
- **Fallback gracieux** : Toujours retourner HTTP 200 avec fallback (pas d'erreur 500) ‚Üí service optionnel, pas de casse
- **Hints comme fallback** : Si RLM indisponible, utiliser hints uniquement (s√©lection manuelle pr√©serv√©e)
- **Message non-bloquant** : Toast ou banner (pas modal) ‚Üí utilisateur peut continuer

**R√©utilisation Code:**
- **ContextBuilder** : `build_context_json()` fonctionne avec hints uniquement (pas de changement requis)

**References:** ADR-005 (RLM Context Selector - Fallback Gracieux), FR8 (Fallback Gracieux), NFR3 (Reliability fallback gracieux)

---

## Epic Summary

**Epic 15** impl√©mente un service optionnel RLM (Recursive Language Models) pour s√©lection automatique intelligente de contexte GDD, r√©duisant friction utilisateur (10+ clics ‚Üí 1 clic) et tokens contexte (20k+ ‚Üí 12-15k Phase 1) sans perte pertinence. Le service est optionnel (toggle on/off), s'int√®gre avec `ContextBuilder` sans casser invariants, et g√®re gracieusement les √©checs RLM (fallback hints uniquement).

**Stories** : 15.1 (Service RLM + Outils GDD), 15.2 (Extension ContextFieldManager), 15.3 (Endpoint API), 15.4 (Toggle UI + Justifications), 15.5 (Mode Override + Lock), 15.6 (Fallback Gracieux + Tests E2E).

**Valeur utilisateur** : R√©duction friction s√©lection contexte, r√©duction tokens contexte sans perte pertinence, expliquabilit√© (justifications claires), contr√¥le utilisateur (override/lock).

**Priorit√©** : Nice-to-have (service optionnel, pas de d√©pendances bloquantes).
