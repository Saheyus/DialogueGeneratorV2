---
description: LLM/OpenAI — SDK v1 AsyncOpenAI, config, tests sans réseau, GPT-5+ Responses API
globs: ["llm_client.py", "llm_client/**/*.py", "factories/**/*.py", "prompt_engine.py", "config/**/llm*.json", "llm_config.json"]
alwaysApply: false
---

- OpenAI: respecter le SDK v1 (ex: `AsyncOpenAI`) et éviter les patterns legacy.
- Ne jamais logguer de secrets. Les clés viennent des variables d'environnement/config (ex: `OPENAI_API_KEY`).
- Tests: aucune requête réseau; utiliser `DummyLLMClient` ou mocks.
- JSON: conserver compatibilité Unity (schémas Pydantic / export) et couvrir par tests.

## GPT-5+ (GPT-5.2, GPT-5.2-pro)

**API différente**: Utilise Responses API (`client.responses.create`) au lieu de Chat Completions (`client.chat.completions.create`). Détermination: modèles commençant par `gpt-5.2` → Responses API.

**Format de requête**:
- `input` au lieu de `messages` (même structure de messages)
- `max_output_tokens` au lieu de `max_tokens`/`max_completion_tokens`
- `tools`: format plat (`{"type": "function", "name": "...", "parameters": {...}}`) au lieu de `{"type": "function", "function": {...}}`
- `tool_choice`: format `{"type": "allowed_tools", "mode": "required", "tools": [...]}` au lieu de `{"type": "function", "function": {...}}`

**Reasoning (phase réflexive)**:
- Disponible uniquement via Responses API (donc uniquement pour GPT-5.2/+)
- Paramètres: `reasoning.effort` (none/low/medium/high/xhigh), `reasoning.summary` (None/"auto"/"detailed")
- Si `reasoning.effort` défini sans `summary`, `summary="auto"` activé automatiquement
- Réponse: `reasoning_trace` extrait depuis `response.reasoning` et `response.output` (item `type="reasoning"`)

**Temperature**: Supportée uniquement si `reasoning.effort` == "none" (ou non spécifié). Ignorée si `reasoning.effort` est défini.

**Parsing de réponse**:
- Responses API: extraire depuis `response.output` (liste d'items, chercher `type="function_call"` ou `type="tool_call"`)
- Chat Completions: extraire depuis `response.choices[0].message.tool_calls`
- Usage tokens: `response.usage.input_tokens`/`output_tokens` (Responses) vs `prompt_tokens`/`completion_tokens` (Chat)

**Modèles GPT-5.2**: `gpt-5.2`, `gpt-5.2-pro`. Modèles GPT-5 (mini/nano) utilisent Chat Completions mais avec `max_completion_tokens`.

**Documentation complète**: Voir `docs/OPENAI_API_GPT5.md` pour détails complets, exemples de code, et références.
